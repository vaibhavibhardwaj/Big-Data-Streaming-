{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 - Data processing for Big Data\n",
    "Name : Vaibhavi Bhardwaj\n",
    "\n",
    "Student Id : 30154987\n",
    "\n",
    "Email id : vbha0006@student.monash.edu\n",
    "\n",
    "## 3. Streaming application using Spark Structured Streaming (60%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SparkSession is created using a SparkConf object, which would use two local cores with a proper application name, and use UTC as the timezone (4%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising Spark and importing files\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming application Analysis in Spark\") \\\n",
    "    .master(\"local[2]\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. From the Kafka producers in Task 1.1 and 1.2, ingest the streaming data into Spark Streaming for both process and memory activities(3%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ingesting Process Data from producer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"A2BprocessStream\"# topic of the  process producer for streaming application\n",
    "\n",
    "# ingesting data\n",
    "df_process = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_process.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ingesting Memory Data from producer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"A2BmemoryStream\"# topic of the memory producer for streaming application\n",
    "\n",
    "#Ingesting Data\n",
    "df_memory = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_memory.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Then the streaming data format should be transformed into the proper formats\n",
    "following the metadata file schema for both process and memory, similar to\n",
    "assignment 2A\n",
    "4\n",
    "(3%)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the value from the producer data as string\n",
    "df_process = df_process.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "#Define the schema for the structured datastream received\n",
    "schema_process = ArrayType(StructType([    \n",
    "    StructField('sequence', StringType(), True), \n",
    "    StructField('machine', StringType(), True), \n",
    "    StructField('PID', StringType(), True), \n",
    "    StructField('TRUN', StringType(), True), \n",
    "    StructField('TSLPI', StringType(), True), \n",
    "    StructField('TSLPU', StringType(), True), \n",
    "    StructField('POLI', StringType(), True), \n",
    "    StructField('NICE', StringType(), True), \n",
    "    StructField('PRI', StringType(), True), \n",
    "    StructField('RTPR', StringType(), True), \n",
    "    StructField('CPUNR', StringType(), True), \n",
    "    StructField('Status', StringType(), True), \n",
    "    StructField('EXC', StringType(), True), \n",
    "    StructField('State', StringType(), True), \n",
    "    StructField('CPU', StringType(), True), \n",
    "    StructField('CMD', StringType(), True), \n",
    "    StructField('ts', StringType(), True)      \n",
    "]))\n",
    "\n",
    "# Unnesting the dataframees \n",
    "df_process = df_process.select(F.from_json(F.col(\"value\").cast(\"string\"), schema_process).alias('parsed_value'))\n",
    "df_process = df_process.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value')) \n",
    "\n",
    "# Renaming the columns for simplicity and according to the description\n",
    "df_formated_process = df_process.select(\n",
    "                    F.col(\"unnested_value.sequence\").alias(\"sequence\"),\n",
    "                    F.col(\"unnested_value.machine\").alias(\"machine\"),\n",
    "                    F.col(\"unnested_value.PID\").alias(\"PID\") ,                   \n",
    "                    F.col(\"unnested_value.TRUN\").alias(\"TRUN\"),\n",
    "                    F.col(\"unnested_value.TSLPI\").alias(\"TSLPI\"),\n",
    "                    F.col(\"unnested_value.TSLPU\").alias(\"TSLPU\"),                    \n",
    "                    F.col(\"unnested_value.POLI\").alias(\"POLI\"),\n",
    "                    F.col(\"unnested_value.NICE\").alias(\"NICE\"),\n",
    "                    F.col(\"unnested_value.PRI\").alias(\"PRI\") ,                   \n",
    "                    F.col(\"unnested_value.RTPR\").alias(\"RTPR\"),\n",
    "                    F.col(\"unnested_value.CPUNR\").alias(\"CPUNR\"),\n",
    "                    F.col(\"unnested_value.Status\").alias(\"Status\"),\n",
    "                    F.col(\"unnested_value.EXC\").alias(\"EXC\"),\n",
    "                    F.col(\"unnested_value.State\").alias(\"State\"),\n",
    "                    F.col(\"unnested_value.CPU\").alias(\"CPU\"),\n",
    "                    F.col(\"unnested_value.CMD\").alias(\"CMD\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the value from the producer data as string\n",
    "df_memory = df_memory.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "#Define the schema for the structured datastream received\n",
    "schema_process = ArrayType(StructType([    \n",
    "    StructField('sequence', StringType(), True), \n",
    "    StructField('machine', StringType(), True), \n",
    "    StructField('PID', StringType(), True), \n",
    "    StructField('MINFLT', StringType(), True), \n",
    "    StructField('MAJFLT', StringType(), True), \n",
    "    StructField('VSTEXT', StringType(), True), \n",
    "    StructField('VSIZE', StringType(), True), \n",
    "    StructField('RSIZE', StringType(), True), \n",
    "    StructField('VGROW', StringType(), True), \n",
    "    StructField('RGROW', StringType(), True), \n",
    "    StructField('MEM', StringType(), True),\n",
    "    StructField('CMD', StringType(), True), \n",
    "    StructField('ts', StringType(), True)      \n",
    "]))\n",
    "\n",
    "\n",
    "# Unnesting the dataframees \n",
    "df_memory = df_memory.select(F.from_json(F.col(\"value\").cast(\"string\"), schema_process).alias('parsed_value'))\n",
    "df_memory = df_memory.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value')) \n",
    "\n",
    "# Renaming the columns for simplicity and according to the description\n",
    "df_memory = df_memory.select(\n",
    "                    F.col(\"unnested_value.sequence\").alias(\"sequence\"),\n",
    "                    F.col(\"unnested_value.machine\").alias(\"machine\"),\n",
    "                    F.col(\"unnested_value.PID\").alias(\"PID\") ,                   \n",
    "                    F.col(\"unnested_value.MINFLT\").alias(\"MINFLT\"),\n",
    "                    F.col(\"unnested_value.MAJFLT\").alias(\"MAJFLT\"),\n",
    "                    F.col(\"unnested_value.VSTEXT\").alias(\"VSTEXT\"),\n",
    "                    F.col(\"unnested_value.VSIZE\").alias(\"VSIZE\"),\n",
    "                    F.col(\"unnested_value.RSIZE\").alias(\"RSIZE\"),\n",
    "                    F.col(\"unnested_value.VGROW\").alias(\"VGROW\") ,                   \n",
    "                    F.col(\"unnested_value.RGROW\").alias(\"RGROW\"),\n",
    "                    F.col(\"unnested_value.MEM\").alias(\"MEM\"),\n",
    "                    F.col(\"unnested_value.CMD\").alias(\"CMD\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Then the streaming data format should be transformed into the proper formats following the metadata file schema for both process and memory, similar toassignment 2A4 (3%)\n",
    "\n",
    "### 4. For process and memory, respectively, create a new column “CMD_PID” concatenating “CMD” and “PID” columns, and a new column “event_time” as timestamp format based on the unix time in “ts” column (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sequence: integer (nullable = true)\n",
      " |-- machine: integer (nullable = true)\n",
      " |-- PID: integer (nullable = true)\n",
      " |-- TRUN: integer (nullable = true)\n",
      " |-- TSLPI: integer (nullable = true)\n",
      " |-- TSLPU: integer (nullable = true)\n",
      " |-- POLI: string (nullable = true)\n",
      " |-- NICE: integer (nullable = true)\n",
      " |-- PRI: integer (nullable = true)\n",
      " |-- RTPR: integer (nullable = true)\n",
      " |-- CPUNR: integer (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- EXC: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- CPU: integer (nullable = true)\n",
      " |-- CMD: string (nullable = true)\n",
      " |-- ts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Transforming the data \n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "# .................................................question 3 changes .....................................................\n",
    "# Reassigning the columns types to integer \n",
    "df_formated_process1 = df_formated_process.withColumn(\"sequence\",F.col(\"sequence\").cast(IntegerType()))\\\n",
    "    .withColumn(\"machine\",F.col(\"machine\").cast(IntegerType()))\\\n",
    "    .withColumn(\"PID\",F.col(\"PID\").cast(IntegerType()))\\\n",
    "    .withColumn(\"TRUN\",F.col(\"TRUN\").cast(IntegerType()))\\\n",
    "    .withColumn(\"TSLPI\",F.col(\"TSLPI\").cast(IntegerType()))\\\n",
    "    .withColumn(\"TSLPU\",F.col(\"TSLPU\").cast(IntegerType()))\\\n",
    "    .withColumn(\"NICE\",F.col(\"NICE\").cast(IntegerType()))\\\n",
    "    .withColumn(\"PRI\",F.col(\"PRI\").cast(IntegerType()))\\\n",
    "    .withColumn(\"RTPR\",F.col(\"RTPR\").cast(IntegerType()))\\\n",
    "    .withColumn(\"CPUNR\",F.col(\"CPUNR\").cast(IntegerType()))\\\n",
    "    .withColumn(\"EXC\",F.col(\"EXC\").cast(IntegerType()))\\\n",
    "    .withColumn(\"CPU\",F.col(\"CPU\").cast(IntegerType()))\\\n",
    "    .withColumn(\"ts\",F.col(\"ts\").cast(IntegerType()))\n",
    "\n",
    "# printing the changed schema\n",
    "df_formated_process1.printSchema()\n",
    "\n",
    "# fixing the relationship between 'NICE' and 'PRI'\n",
    "# NICE = PRI-120 if PRI is not equal to 0 , else NICE is 0\n",
    "df_formated_process1 = df_formated_process1.withColumn(\"NICE\", F.when(F.col(\"PRI\")==0, 0).otherwise(F.col(\"PRI\")-120))\n",
    "\n",
    "\n",
    "# .................................................question 4 changes .....................................................\n",
    "# making the CMD_PID columns \n",
    "df_formated_process1 = df_formated_process1.withColumn(\"CMD_PID\", concat(F.col('CMD'),F.col(\"PID\")))\n",
    "# Making event column by changing ts to timestamp \n",
    "df_formated_process1 = df_formated_process1.withColumn('event_time', \\\n",
    "                                                       from_unixtime(col('ts')).cast(TimestampType()))\n",
    "# with 20 second water mark \n",
    "df_formated_process1 = df_formated_process1.withWatermark(\"event_time\", \"20 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit,when , expr\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "# .................................................question 3 changes .....................................................\n",
    "# Fixing the values of string to number \n",
    "# removing 'K','G' and 'M' from the value in integer cols of memory \n",
    "# k is kilo so 1000\n",
    "# m is million so 1000000\n",
    "df_memory = df_memory.withColumn('MINFLT',when(col('MINFLT').contains('K'),expr('substring(MINFLT,1,length(MINFLT)-1)').\\\n",
    "                                              cast('double')*1000).otherwise(col('MINFLT').cast('double')))\n",
    "\n",
    "\n",
    "df_memory = df_memory.withColumn('MINFLT',when(col('MAJFLT').contains('M'),expr('substring(MAJFLT,1,length(MAJFLT)-1)').\\\n",
    "                                              cast('double')*1000000).otherwise(col('MAJFLT').cast('double')))\n",
    "\n",
    "df_memory = df_memory.withColumn('MINFLT',when(col('VSTEXT').contains('K'),expr('substring(VSTEXT,1,length(VSTEXT)-1)').\\\n",
    "                                              cast('double')*1000).otherwise(col('VSTEXT').cast('double')))\n",
    "\n",
    "df_memory = df_memory.withColumn('MINFLT',when(col('RSIZE').contains('K'),expr('substring(RSIZE,1,length(RSIZE)-1)').\\\n",
    "                                              cast('double')*1000).\\\n",
    "                                 when(col('RSIZE').contains('M'),expr('substring(RSIZE,1,length(RSIZE)-1)').\\\n",
    "                                              cast('double')*1000000).otherwise(col('RSIZE').cast('double')))\n",
    "\n",
    "df_memory = df_memory.withColumn('MINFLT',when(col('VGROW').contains('K'),expr('substring(VGROW,1,length(VGROW)-1)').\\\n",
    "                                              cast('double')*1000).otherwise(col('VGROW').cast('double')))\n",
    "\n",
    "df_memory = df_memory.withColumn('MINFLT',when(col('RGROW').contains('K'),expr('substring(RGROW,1,length(RGROW)-1)').\\\n",
    "                                              cast('double')*1000).otherwise(col('RGROW').cast('double')))\n",
    "\n",
    "\n",
    "# Now changing the datatypes of string or double to string \n",
    "df_memory_formatted1 = df_memory.withColumn(\"sequence\",F.col(\"sequence\").cast(IntegerType()))\\\n",
    "    .withColumn(\"machine\",F.col(\"machine\").cast(IntegerType()))\\\n",
    "    .withColumn(\"PID\",F.col(\"PID\").cast(IntegerType()))\\\n",
    "    .withColumn(\"MINFLT\",F.col(\"MINFLT\").cast(IntegerType()))\\\n",
    "    .withColumn(\"MAJFLT\",F.col(\"MAJFLT\").cast(IntegerType()))\\\n",
    "    .withColumn(\"VSTEXT\",F.col(\"VSTEXT\").cast(IntegerType()))\\\n",
    "    .withColumn(\"VSIZE\",F.col(\"VSIZE\").cast(IntegerType()))\\\n",
    "    .withColumn(\"RSIZE\",F.col(\"RSIZE\").cast(IntegerType()))\\\n",
    "    .withColumn(\"VGROW\",F.col(\"VGROW\").cast(IntegerType()))\\\n",
    "    .withColumn(\"RGROW\",F.col(\"RGROW\").cast(IntegerType()))\\\n",
    "    .withColumn(\"MEM\",F.col(\"MEM\").cast(IntegerType()))\\\n",
    "    .withColumn(\"ts\",F.col(\"ts\").cast(IntegerType()))\n",
    "\n",
    "\n",
    "# .................................................question 4 changes .....................................................\n",
    "\n",
    "# Adding event time column to the dataframe with 20 seconds watermark\n",
    "df_memory_formatted1 = df_memory_formatted1.withColumn('event_time', \\\n",
    "                                                       from_unixtime(col('ts')).cast(TimestampType()))\n",
    "# assigning a 20 secong water mark to event_time\n",
    "df_memory_formatted1 = df_memory_formatted1.withWatermark(\"event_time\", \"20 seconds\")\n",
    "#making the CMD_PID column\n",
    "df_memory_formatted1 = df_memory_formatted1.withColumn(\"CMD_PID\", concat(F.col('CMD'),F.col(\"PID\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query to check in the console \n",
    "query = df_memory_formatted1 \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='20 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Persist the transformed streaming data in parquet format for both process and memory (5%) The process data should be stored in “process.parquet” in the same folder of your notebook, and the memory data should be stored in “memory.parquet” in the same folder of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persisting the transformed data into the parquet dataset\n",
    "# for process it is process.parquet\"\n",
    "query = df_formated_process1 \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", \"process.parquet\")\\\n",
    "    .option(\"checkpointLocation\",\"process.parquet/checkpoint\") \\\n",
    "    .trigger(processingTime='20 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persisting the transformed data into the parquet dataset\n",
    "# for memory it is memory.parquet\"\n",
    "query = df_memory_formatted1 \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", \"memory.parquet\")\\\n",
    "    .option(\"checkpointLocation\",\"memory.parquet/checkpoint\") \\\n",
    "    .trigger(processingTime='20 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Load the machine learning models given , and use the models to predict whether each process or memory streaming record is an attack event, respectively (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the models by importing PipelineModel\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# process model\n",
    "processModel = PipelineModel.load('process_pipeline_model')\n",
    "# memory model \n",
    "memoryModel = PipelineModel.load('memory_pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values for the streaming ingeste data \n",
    "processPredictions = processModel.transform(df_formated_process1) # tranforming process data\n",
    "memoryPredictions = memoryModel.transform(df_memory_formatted1) # tranforming memory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Using the prediction result, and monitor the data following the requirements below (30%)\n",
    "\n",
    "A. If any program in one machine is predicted as an attack in EITHER process or memory activity prediction, it could be a false alarm or a potential attack. Keep track of the approximate count of such events in every 2-min window for each machine for process and memory, respectively, and write the stream into Spark Memory sink using complete mode.\n",
    "    - Your aggregated result should include machine ID, the time window, and the counts\n",
    "    - Note that if there are more than one entries having the SAME “CMD_PID” in a 2-min window, get the approximate distinct count\n",
    "    - For example, if two or more records of “atop” program with the exact same “CMD_PID” are predicted as an attack in the process between 2020-10-10 10:10:10 and 2020-10-10 10:11:09 , only need to count this “atop” program attack once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.a Memory \n",
    "# importing libraries needed by this cell for the stated tranformation\n",
    "from pyspark.sql.functions import col, window \n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "# windowing the tranformed after passing through the model \n",
    "# 2 min window, with machine id and total attacks \n",
    "# prediction = 1 is an attack and unique CMD_PID or cmd+pid\n",
    "windowedCountsmemory = memoryPredictions.where(col('prediction')==1)\\\n",
    "    .groupBy(window(memoryPredictions.event_time, \"120 seconds\"),'machine')\\\n",
    "    .agg(func.approx_count_distinct(concat(F.col('CMD'),F.col('PID'))).alias(\"total\"))\\\n",
    "    .select(\"window\",'machine','total')\n",
    "\n",
    "# Making a query to run the data and save it in the memory output \n",
    "query7amemory = windowedCountsmemory \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"7aquerym\") \\\n",
    "    .option(\"truncate\",\"false\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the query \n",
    "query7amemory.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.a Process \n",
    "# implementing the same as above but on process data \n",
    "windowedCountsprocess = processPredictions \\\n",
    "    .where(col('prediction')==1)\\\n",
    "    .groupBy(window(processPredictions.event_time, \"120 seconds\"),'machine')\\\n",
    "    .agg(func.approx_count_distinct(F.col('CMD_PID')).alias(\"total\"))\\\n",
    "    .select(\"window\",'machine','total')\n",
    "\n",
    "# Making a query to run the data and save it in the memory output \n",
    "query7aprocess = windowedCountsprocess \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"7aqueryp\") \\\n",
    "    .option(\"truncate\",\"false\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the query \n",
    "query7aprocess.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. If a program in one machine, having the same “CMD” and “PID” in both\n",
    "process and memory streaming data, is predicted as an attack in BOTH\n",
    "process and memory activity prediction, then this is considered as an attack\n",
    "event. Find the streaming events fulfilling the criteria, create a new column to\n",
    "record the processing time\n",
    "8 and persist them in parquet.\n",
    "- Note the program with the same “CMD” and “PID” might not be\n",
    "generated at the exact same event time. If the difference between the\n",
    "event times in process and memory is less than 30 seconds and the\n",
    "program fulfills the criteria of matching “CMD” and “PID”, then you\n",
    "should include them for the above checking.\n",
    "- If there are multiple entries fulfilling the above criteria in process or\n",
    "memory, do not remove the extra entries\n",
    "- Persist the program’s relevant information (including process &\n",
    "memory data, process & memory’s event and processing timestamp,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.b \n",
    "# Taking the data needed to implement a join between the 2 streaming datasets\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# selecting rows needed to be used further for process \n",
    "# changing the column names for removing ambiguity between common fields between data from process and memory\n",
    "processWithWatermark = processPredictions \\\n",
    "  .selectExpr(\"CMD AS CMDpre\", \"PID AS PIDpre\",\"event_time AS event_timePre\",'TRUN', \\\n",
    "              'TSLPI', 'TSLPU', 'POLI', 'NICE', 'PRI', 'RTPR', 'CPUNR', 'Status', 'EXC', 'State', 'CPU', \\\n",
    "               'ts  AS tsPre', 'CMD_PID','prediction AS proPrediction','sequence', 'machine')\n",
    "\n",
    "# selecting rows needed to be used further for memory  \n",
    "# changing the column names for removing ambiguity between common fields between data from process and memory\n",
    "memoryWithWatermark = memoryPredictions \\\n",
    "  .selectExpr(\"CMD\", \"PID\" ,\"event_time\",'sequence as sequenceMem', 'machine as machineMem', 'MINFLT', \\\n",
    "              'MAJFLT', 'VSTEXT', 'VSIZE', 'RSIZE', 'VGROW', 'RGROW', 'MEM', 'ts  as tsMem', 'event_time','prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join with time range conditions\n",
    "# inner joining the dataset and putting conditions as asked in the question statement\n",
    "# CMD , PID of process and memory should be same \n",
    "# process event time should be within 30 sec of memory event  \n",
    "TEST = processWithWatermark.join( memoryWithWatermark, expr(\"\"\" \n",
    "      CMDpre = CMD AND \n",
    "      PID =  PIDpre AND\n",
    "      proPrediction = 1 AND\n",
    "      prediction = 1 AND\n",
    "      (event_timePre <= event_time + interval 30 seconds OR\n",
    "      event_time <= event_timePre + interval 30 seconds)\n",
    "      \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing in the console for checking \n",
    "### for debugging purpose \n",
    "query = TEST.select(\"event_time\",\"CMD\",\"PID\",\"event_timePre\")\\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='20 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the query \n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the a new column process_time to the dataset \n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "TEST = TEST.withColumn('processing_time',current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the dataset into a parquet file\n",
    "# finally putting the data from Process and memory to put in the parquet file \n",
    "# data from process and memory is taken further to the parquet file \n",
    "query = TEST.select(\"CMD\", \"PID\" ,'sequenceMem', 'machineMem','machine','sequence', 'MINFLT','TRUN', \\\n",
    "              'TSLPI', 'TSLPU', 'POLI', 'NICE', 'PRI', 'RTPR', 'CPUNR', 'Status', 'EXC', 'State', 'CPU','CMD_PID',\n",
    "              'MAJFLT', 'VSTEXT', 'VSIZE', 'RSIZE', 'VGROW', 'RGROW', 'MEM', 'tsMem','tsPre','event_time','event_timePre',\\\n",
    "                    'prediction','processing_time') \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", \"process_memory_attack.parquet\")\\\n",
    "    .option(\"checkpointLocation\",\"process_memory_attack.parquet/checkpoint\") \\\n",
    "    .trigger(processingTime='20 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preinting in the console for checking with at the strcture of the data \n",
    "query = TEST.select(\"CMD\", \"PID\" ,'sequenceMem', 'machineMem','machine','sequence', 'MINFLT','TRUN', \\\n",
    "              'TSLPI', 'TSLPU', 'POLI', 'NICE', 'PRI', 'RTPR', 'CPUNR', 'Status', 'EXC', 'State', 'CPU','CMD_PID',\n",
    "              'MAJFLT', 'VSTEXT', 'VSIZE', 'RSIZE', 'VGROW', 'RGROW', 'MEM', 'tsMem','tsPre','event_time','event_timePre',\\\n",
    "                    'prediction','processing_time') \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='20 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Visualise the data in line charts for step 7a (5%)\n",
    "    - For the count of suspect attacks for each machine in step 7a, use Spark SQL to query the data from Spark Memory sink, and prepare a line chart plot for showing the count of suspect attacks for each machine at each 2-min window from the start to the most recent, and refresh the plot every 10 minutes\n",
    "    - Hint - x-axis can be used to represent the timeline, while y-axis can be used to represent the count; each machine’s line data can be represented in different color legends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_plots():\n",
    "    try:\n",
    "        width = 9.5\n",
    "        height = 6\n",
    "        fig = plt.figure(figsize=(width,height)) # create new figure\n",
    "        ax = fig.add_subplot(111) # adding the subplot axes to the given grid position\n",
    "        fig.suptitle('Real-time uniform stream data visualization') # giving figure a title\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Value')        \n",
    "        fig.show() # displaying the figure\n",
    "        fig.canvas.draw() # drawing on the canvas\n",
    "        return fig, ax\n",
    "    except Exception as ex:\n",
    "        print(str(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig, ax = init_plots()\n",
    "\n",
    "while True:\n",
    "    df = spark.sql(\"select * from 7aqueryp\").toPandas()\n",
    "    x4 =df[df['machine']==4] \n",
    "   \n",
    "    x = df['minute_bin'].to_list()\n",
    "    y = df['Total Impressions'].to_list() \n",
    "    ax.clear()\n",
    "    print(x)\n",
    "    print(y)\n",
    "    ax.plot(x, y)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Impressions')    \n",
    "    fig.canvas.draw()\n",
    "   \n",
    "        \n",
    "    time.sleep(1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
